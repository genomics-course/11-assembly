{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 11.1: *de-novo* Genome assembly \n",
    "\n",
    "\n",
    "### Learning objectives: \n",
    "\n",
    "By the end of this notebook you will:\n",
    "\n",
    "+ Be familiar with assembled genome file formats. \n",
    "+ Know where to find assembled genomes and their raw files online.\n",
    "+ Be able to compute genome size from kmer counts of Illumina short reads. \n",
    "+ Experience running Illumina, and Illumina+PacBio hybrid *de novo* assemblies.  \n",
    "\n",
    "### Associated readings: \n",
    "\n",
    "+ Li, Fay-Wei, and Alex Harkess. “A Guide to Sequence Your Favorite Plant Genomes.” Applications in Plant Sciences 6, no. 3 (n.d.): e1030. https://doi.org/10.1002/aps3.1030.\n",
    "\n",
    "### Other resources:\n",
    "\n",
    "**You do not need to read this paper, but its data will be used as a reference in this notebook:**\n",
    "\n",
    "+ Sumpter, Nicholas, Margi Butler, and Russell Poulter. “Single-Phase PacBio De Novo Assembly of the Genome of the Chytrid Fungus Batrachochytrium Dendrobatidis, a Pathogen of Amphibia.” Microbiology Resource Announcements 7, no. 21 (November 2018). https://doi.org/10.1128/MRA.01348-18.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software used in this notebook\n",
    "We will use three Python packages and also a number of command-line programs that have been installed using the conda package manager. The commands to install the software are shown below for reference, but are commented out. You do not need to run the installation commands since the software is already installed for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toyplot\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install bioconda::sra-tools   # download fastq data\n",
    "# conda install bioconda::cutadapt    # read adapter/quality trimmer\n",
    "# conda install bioconda::jellyfish   # kmer counting\n",
    "# conda install bioconda::spades      # debruijn graph assembler\n",
    "# conda install bioconda::quast       # compare assemblies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational resources for genome assembly\n",
    "\n",
    "The process of assembling genomes *de novo*, meaning without any *a priori* guide, can be a very computationally intensive task. In particular, it requires large amounts of RAM (i.e., accessible memory). This type of resource is not easily shared across nodes of a computing cluster (e.g., a University HPC system), and so instead clusters typically contain a few special nodes, designated *high-memory* nodes, that are  best for this process. As an example, even small bacterial genomes will typically require 8-16GB of RAM, which is the max for most laptops. A human genome may require 100-500Gb, depending on the software being used (e.g., canu). Much larger genomes, or those with many repeats (e.g., plant genomes), often require much more memory still. High memory nodes typically have 1-2Tb of RAM or more, which should usually be sufficient.\n",
    "\n",
    "In this notebook and the next we will complete several genome assemblies using real data. However, to avoid the need for significant computational resources most of the computationally intensive code is commented out, meaning you will not actually run it. We've made the results files available to you so that you can learn how the data and results are represented, and to interpret their format and meaning.\n",
    "\n",
    "For these exercises we will work with the relatively small  genome of *Batrachochytrium dendrobatidis*, a Chytrid fungus that is an ecologically important pathogen of amphibians. This is a eukaryotic genome that has data available in a number of formats. We will look at both Illumina and PacBio data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Bd* genome\n",
    "We will assemble the *Bd* chytrid fungus genome from a paper by Sumpter et al. (2018). I selected this paper because it provides a very concise report of a genome assembly and also where to find the associated data. You can find a link at the top of this notebook.\n",
    "\n",
    "The first draft genome assembly of the *Bd* genome was sequenced by the US DOE Joint Genome Institute using Sanger sequencing to 8.74X coverage in 2011. That assembly is 23.8Mb in size, in 62 scaffolds (N50 scaffold size of 1.5Mb) and 425 contigs (N50 contig size 318Kb). See here for summary of the Sanger-based assembly (https://www.ncbi.nlm.nih.gov/assembly/GCF_000203795.2/). This genome is considered a small by relative standards, and the quality of it is also low by current standards.\n",
    "\n",
    "In the paper by Sumpter et al. a new draft genome is assembled using PacBio data alone (and some additional scaffolding methods). Our exercise in this notebook will be to re-assemble this data set using their data, and to explore alternative assembly methods. How good of an assembly of the *Bd* genome can we get from short read data alone? How does this compare to long-read only or hybrid assembly approaches?\n",
    "\n",
    "By browsing NCBI we can find the assembly statistics for the Sumpter et al. assembly (https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_003595275.1/). Moreover, we can see that several other genome assemblies have also been completed for this same species even more recently (https://www.ncbi.nlm.nih.gov/datasets/genome/?taxon=109871)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>[1] Question:</b>\n",
    "    Select a number of statistics from the assembly summary statistics of the DOE 2011 and Sumpter 2018 datasets (found from the links above) and describe how these statistics indicate which assembly is higher quality and why. Answer in markdown below.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download sequence data (.fastq) files\n",
    "\n",
    "The paper by Sumpter et al. includes a section called *Data availability* describing the accession number under which the data have been archived. This is typical for any genome paper. The genome assembly is deposited under the accession ID `QUAD00000000`, with the particular *version* of the genome assembled in this paper given the ID QUAD01000000. Future assemblies of the same data, perhaps using better software, could be uploaded and associated with this project, but given new accession IDs. This accession identifier can be used to query the assembly and retrieve the contigs which are labeled QUAD01000001-QUAD01000063. Follow the link in the paper to the genome project page [https://www.ncbi.nlm.nih.gov/nuccore/QUAD00000000](https://www.ncbi.nlm.nih.gov/nuccore/QUAD00000000). There you can find additional links to the Project accession, or the BioSample accession, all of which can tell you more about the project and how to access the data. As described in the Data Availability section of the paper, there were 10 other strains of Bd also sequenced in this study (using Illumina short read data) which are also linked under the same Project ID (PRJNA483086), but are not linked under the same Genome Assembly (QUAD00000000). By following links on the project page we can find pages for the sequenced individuals (SRXs) and sequencing runs (SRRs). I know, the many types of accession IDs can be hard to keep straight. These last IDs link to the actual sequence data files. For example, this page has details about the PacBio data we will be assembling: [https://www.ncbi.nlm.nih.gov/sra/SRX4676189[accn]](https://www.ncbi.nlm.nih.gov/sra/SRX4676189[accn]). \n",
    "\n",
    "We have installed a suite of command-line tools, called the `sra-tools`, that can be used to query NCBI and other databases using accession IDs to download sequence data files and convert them to fastq format. The most relevant tool from this set is called  `fastq-dump`. The commands below will download the PacBio data for Bd, and Illumina data for one of the other strains included in the study which was shown to be identical to the one sequenced with PacBio. The PacBio data is all single reads, whereas the Illumina data is paired-end reads (separate R1 and R2 files). The PabBio file (`SRR7825134.fastq`) is an uncompressed fastq file that is 2.3Gb in size, and the Illumina data is in two files (`SRR7825135_1.fastq` and `SRR7825135_2.fastq`) each 3.6Gb as uncompressed fastq.\n",
    "\n",
    "<span style=\"color:red\">*This download takes ~20 minutes* and so instead of having you do it I've instead made the files already available to you in your current directory.</span> As we've seen before, you could navigate through the NCBI website to find a download link for these accessions. But, more commonly, data are downloaded using a tool published by NCBI called `fastq-dump`. This code below is included to show an example of how to download data using fastq-dump and an accession ID. It is commented out because you do not need to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# # Download Bd RTP6 PacBio reads\n",
    "# fastq-dump SRR7825134\n",
    "\n",
    "# # Download Bd RTP5 Illumina HiSeq 2000 paired-end reads\n",
    "# fastq-dump SRR7825135 --split-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fastq data files\n",
    "You have seen fastq data files before. The PacBio data comes in the same format as Illumina fastq data files, the reads are just longer. You can look at the download pages for the two data sets to find some summary information: pacbio [https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR7825134](https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR7825134), and Illumina [https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR7825135](https://trace.ncbi.nlm.nih.gov/Traces/sra/?run=SRR7825135). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>[2] Question:</b> \n",
    "    How many sequenced reads are in each of the raw fastq data files (SRR7825134.fastq, SRR7825135_1.fastq and SRR7825135_2.fastq)? Find this information in the URL links above and answer in the Markdown cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PacBio read lengths\n",
    "In addition to the number of reads, we're also interested in the lengths of reads for the PacBio data, since this can vary among data sets. The commented out code below uses Python to examine the read length distribution in the PacBio data. The average read length is around 7500bp in length (7.5kb) and many reads longer than 20kb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lengths of each read\n",
    "with open(\"/tmp/SRR7825134.fastq\") as indata:\n",
    "    lengths = np.array([len(next(indata)) for line in indata if line[0] == \"@\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot as a barplot\n",
    "toyplot.bars(\n",
    "    np.histogram(lengths, bins=20),\n",
    "    width=350, \n",
    "    height=300,\n",
    "    xlabel=\"read length\", \n",
    "    ylabel=\"number of reads\",\n",
    "    label=\"avg. read length={:.0f}\".format(np.mean(lengths))\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim reads for quality and adapters\n",
    "\n",
    "Filtering and trimming is an important step for any genome assembly. The cleaner your reads are the better and faster the genome will assemble. The command below uses the program `cutadapt` to trim Illumina adapters from the reads and trim from the 3' end when quality falls below a set threshold based on the quality scores in the fastq data files. To keep our notebook easy to read we redirect the outputs normally printed to screen by `cutadapt` and write it to a file instead. This includes stats about how many reads were trimmed or removed. The trimming step here takes only a few minutes, but again, we've commented it out for efficiency. \n",
    "\n",
    "We will not clean up the PacBio data files for now. These are expected to have lower quality scores, and so instead of trimming them to throw away the bad parts it is more common to try to *correct* the errors either as a first step before assembly, or after assembly. This can involve mapping short-read or long-reads to the long-reads before assembly, or to the assembled contigs after assembly. \n",
    "\n",
    "Let's start by trimming the short read data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutadapt parameters explained:\n",
    "# -q: trim 3' when quality < 10\n",
    "# -a: trim adapters from R1\n",
    "# -A: trim adapters from R2\n",
    "# -o: output R1 filename\n",
    "# -p: output R2 filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# cutadapt \\\n",
    "#     -q 10 \\\n",
    "#     -a AGATCGGAAGAGC \\\n",
    "#     -A AGATCGGAAGAGC \\\n",
    "#     -o SRR7825135_1.trim.fastq \\\n",
    "#     -p SRR7825135_2.trim.fastq \\\n",
    "#     SRR7825135_1.fastq \\\n",
    "#     SRR7825135_2.fastq > trim-report.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report produced by `cutadapt` includes statistics about how many reads were trimmed based on low quality or adapter contamination. Open the report in a new tab from ([this link](https://genomics-course.github.io/11-assembly/notebooks/trim-report.txt)) to see the results. In the command above we named the new trimmed read files `SRR7825135_1.trim.fastq` and `SRR7825135_2.trim.fastq`. These are the files we will use going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>[3] Question:</b> \n",
    "    From the statistics in the cutadapt report (see link above), what percentage of read1 and read2s were trimmed for adapters? How many bases in total were trimmed based on quality scores?  Answer in markdown below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genome size and complexity\n",
    "Your assigned reading by Li and Harkess titled \"A guide to sequence your favorite plant genomes\" describes some particular difficulties of assembling plant genomes, but more broadly the advice from this paper can be generalized to many other organisms with difficult to assemble genomes as well. They make the important point that before starting any genome sequencing project it is important to learn as much as you can about your organism, as this should inform the type of approach that you take, and your expectations about the result you will get. \n",
    "\n",
    "One of the first things you can do when you get some initial shotgun sequenced Illumina data is to investigate some of these same questions: How big is the genome, how variable, and what ploidy (has it experienced genome duplications)? It turns out we can learn answers to these questions by examining the distribution of kmer counts in the raw read data itself, without even having to map, align, or assemble it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jellyfish: kmer counting tool\n",
    "The [jellyfish](https://github.com/gmarcais/Jellyfish/tree/master/doc) program is a tool used to efficiently count kmers from a genome fasta file or fastq sequenced read files. It works much faster than the Python code we wrote last week to find all kmers in a sequence and is quite easy to use. It turns out we can learn a lot from examining the distribution of kmer counts. The quote below is from the Jellyfish paper by the authors, Marçais and Kingford (2011): \n",
    "    \n",
    "<blockquote>\n",
    "\n",
    "Given a string S, we are often interested in counting the number of occurrences in S of every substring of length k. These length-k substrings are called k-mers and the problem of determining the number of their occurrences is called k-mer counting.\n",
    "\n",
    "Counting the k-mers in a DNA sequence is an important step in many applications. For example, genome assemblers using the overlap-layout-consensus paradigm, such as the Celera (Miller et al., 2008; Myers et al., 2000) and Arachne (Jaffe et al., 2003) assemblers, use k-mers shared by reads as seeds to find overlaps. Statistics on the number of occurrences of each k-mer are first computed and used to filter out which k-mers are used as seeds. Such k-mer count statistics are also used to estimate the genome size: if a large fraction of k-mers occur c times, we can estimate the sequencing coverage to be approximately c and derive an estimate of the genome size from c and the total length of the reads. In addition, in most short-read assembly projects, errors are corrected in the sequencing reads to improve the quality of the final assembly. For example, Kelley et al. (2010) use k-mer frequencies to assess the likelihood that a misalignment between reads is a sequencing error or a genuine difference in sequence. A third application is the detection of repeated sequences, such as transposons, which play an important biological role. De novo repeat annotation techniques find candidate regions based on k-mer frequencies (Campagna et al., 2005; Healy et al., 2003; Kurtz et al., 2008; Lefebvre et al., 2003). The counts of k-mers are also used to seed fast multiple sequence alignment (Edgar, 2004). Finally, k-mer distributions can produce new biological insights directly. Sindi et al. (2008) used k-mers frequencies with large k (20 ≤ k ≤ 100) to study the mechanisms of sequence duplication in genomes.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running jellyfish on the raw data files produces a `.jf` formatted file with information about the kmers in the data. This would take a few minutes to run. Again, we've commented it out so you can proceed to the output files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jellyfish parameters explained:\n",
    "# -m kmer size\n",
    "# -s genome size estimate\n",
    "# -t number of threads to use\n",
    "# -C switch to count 'canonical kmers' (reverse complement)\n",
    "# -o output location\n",
    "# <(...) this decompresses the file as it is passed to jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# jellyfish count \\\n",
    "#     -m 21 -s 40M -t 10 -C \\\n",
    "#     -o 21mer.jf \\\n",
    "#     SRR7825135_1.fastq \\\n",
    "#     SRR7825135_2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running jellyfish we can then extract a histogram from the .jf file using the `histo` function, which returns the histogram as a text table showing the number of unique kmers that occurred N times. You can see in the bash script below we direct the histogram to a file named `21mer.hist.csv`. Again we put a version of this CSV file online for you to view [here](https://raw.githubusercontent.com/genomics-course/11-assembly/master/notebooks/21mer.hist.csv) and so that you do not have to compute it yourself. We will load the file from this URL further below to plot the kmer distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# jellyfish histo 21mer.jf > 21mer.hist.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use Python to load the Illumina kmer count data with the Pandas library and plot it with the Toyplot library. We can see clearly that there is a single best peak at around 55X coverage or so. We can assume that this peak represents a single copy of the genome -- all possible unique kmers in the genome have been sequenced to 55X coverage on average -- and that any other peaks represent heterozygous sites in the genome (thus the kmers at those regions only come up half as often), or errors, repetitions, and other such noise. The density of our single-copy peak stretches from about 30 to 80X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the histogram data as a table\n",
    "url = \"https://raw.githubusercontent.com/genomics-course/11-assembly/master/notebooks/21mer.hist.csv\"\n",
    "df = pd.read_csv(url, sep=\" \", names=[\"bins\", \"counts\"])\n",
    "\n",
    "# show the first few rows of data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram\n",
    "toyplot.fill(\n",
    "    df.counts[2:200], \n",
    "    width=500, \n",
    "    height=300,\n",
    "    ylabel=\"number of unique kmers\", \n",
    "    xlabel=\"kmer coverage depth\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genome size estimation\n",
    "We can estimate the genome size by taking the total number of kmers (multiply the coverage times the number of unique kmers in each bin) and dividing this by the mean coverage (peak position). We get an estimate of ~30Mb, which is close to the current estimated value of the *Bd* genome of 24Mb. It's worth noting that kmer based methods for estimating genome size require high coverage of the genome, as we have here, but would not work well if our sequencing depth was much lower, say <30X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate genome size estimate\n",
    "int(np.sum(df.counts[1:] * df.bins[1:]) / 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *de novo* assembly methods\n",
    "\n",
    "There are many programs available today for genome assembly. They vary in terms of requirements of the input data, their speed, RAM requirements, and the resulting accuracy of the assemblies. For some types of data, or sizes of genomes, some tools will work better than others. Because *de novo* assembly is such a time consuming process it is difficult to compare many tools. It's worth reading a few bioinformatics articles that compare assembly tools to learn more about which one might be best for your project. \n",
    "\n",
    "You were assigned an article to read that compares different genome assemblies based for the Vertebrate Genome Project (VGP) using different technologies. We will examine larger vertebrate genomes in the next notebook. Here, we will focus on the problem of accurately assembling a smaller genome, which is typically easier.\n",
    "\n",
    "The assembly tool we will use below, [spades](https://github.com/ablab/spades), has been popular for years, and is continually updated to keep pace with advancing sequencing technologies. It is a *de Bruijn* graph based assembly method that is best for short-read assembly, but which also has support for *hybrid* assembly, which combines short-read contig assembly with scaffolding by using long-reads (HybridSpades). We will try these approaches here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *de novo* assembly with spades: Illumina only\n",
    "\n",
    "The command below runs a *de novo* assembly using Illumina data only. It first runs a pre-processing read correction step, then the main assembly of the data, and finally a post-processing contig gap filling correction step. While it is running it prints a report of its progress to the screen and to a log file. This reports what is happening in each step of the assembly, how long it has been running, and how much RAM it is consuming. Look at the log file ([here](https://genomics-course.github.io/11-assembly/notebooks/assembly_spades/spades.log)) for this assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spades parameters explained:\n",
    "\n",
    "# -k: if not set, then kmer size is automatically selected\n",
    "# -o: output directory name\n",
    "# -1: read1 inputs\n",
    "# -2: read2 inputs\n",
    "# -m: memory limit in Gb\n",
    "# -t: number of CPU threads to use\n",
    "\n",
    "# --only-error-correction: if set, performs read error correction only. \n",
    "# --only-assembler: if set, then performs assembly only without corrections.\n",
    "# --careful: if set, turns on post-processing mismatch-correction (gap filling)\n",
    "# if none of these three are set then only error-correction and assembly are done by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# spades.py \\\n",
    "#     -o assembly_spades/ \\\n",
    "#     -1 SRR7825135_1.trim.fastq \\\n",
    "#     -2 SRR7825135_2.trim.fastq \\\n",
    "#     -m 64 \\\n",
    "#     -t 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>[4] Question:</b> \n",
    "    From looking at the log file (linked to above) for this assembly run with spades, answer the following questions in Markdown below. \n",
    "    What kmer sizes were used in the assembly? How long did the assembly step take to run? If the file is hard for you to read try decreasing the font size and searching for keywords in the text. Answers are near the end.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success, you just assembled a genome\n",
    "If you want, you can look at the assembled contigs in fasta format in the `assembly_spades` results folder. Now let's try another method, by including some additional PacBio data to span the gaps between contigs in the assembly to see if we get longer scaffolds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *de novo* HYBRID assembly with spades: Illumina & PacBio\n",
    "To run spades in hybrid mode we can use the same command as above with just the addition of the `--pacbio` flag (or `--nanopore` if we had nanopore data) and the input fastq file for the long read data. The log file for this assembly can be found [here](https://genomics-course.github.io/11-assembly/notebooks/assembly_spades_hybrid/spades.log). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# spades.py \\\n",
    "#     -o assembly_spades_hybrid/ \\\n",
    "#     -1 SRR7825135_1.fastq \\\n",
    "#     -2 SRR7825135_2.fastq \\\n",
    "#     --pacbio SRR7825134.fastq \\\n",
    "#     -m 64 \\\n",
    "#     -t 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get assembly quality statistics using QUAST\n",
    "The [quast](http://quast.bioinf.spbau.ru/manual.html) program is used to calculate genome quality statistics and compare assemblies. It calculates N50 contig and scaffold sizes (something we've done before), and reports BUSCO scores (number of single-copy genes present) which we've read about before. The code below will download the BUSCO single copy gene set and search for these genes in our assembled genomes. We provide it with the assembled scaffolds from both assemblies and the result will compare the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# # download the busco gene set to this computer\n",
    "# quast-download-busco\n",
    "\n",
    "# # run quast to compare assemblies and find BUSCO conserved genes in both\n",
    "# quast.py \\\n",
    "#     assembly_spades/scaffolds.fasta \\\n",
    "#     assembly_spades_hybrid/scaffolds.fasta \\\n",
    "#     -o quast_results \\\n",
    "#     --conserved-genes-finding \\ \n",
    "#     --fungus\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running quast\n",
    "After running quast you can open a summary of the results in HTML format from the output directory. I've provided a link to it for you that should open in a new browser tab from ([here](https://genomics-course.github.io/11-assembly/notebooks/quast2/report.html)). This provides information on the assembly statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>[5] Question:</b> \n",
    "    What is the N50 contig size, and number of contigs for each assembly? Based on this, which method do you think provided a better genome assembly? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>[6] Question:</b> \n",
    "    Based on the same quast results report, how did the two assemblies differ in terms of their BUSCO scores? Based on either BUSCO score how well assembled do you think this genome is?  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
